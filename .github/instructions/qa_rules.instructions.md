---
description: For every software testing and qa automation related task.
alwaysApply: false
---
Project Development Guidelines

The following guidelines instruct the agent on best practices for project development, testing, code reviews, and implementation. It also specifies the tools available to the agent.

ğŸ¤– Agent & Tooling Guidelines
Task Management
â— Use the `sequentially mcp` to break down complex tasks into smaller, manageable, and actionable steps. This ensures a structured and methodical approach to development and testing.
Manual Testing & Exploration
â— When manual inspection or debugging in a browser is required, use the `playwright mcp`. This is useful for understanding application behavior before writing automated tests or for verifying changes that are difficult to automate.
Technical Documentation & Research
â— For accessing technical documentation, such as the OWASP Application Security Verification Standard (ASVS), OWASP Cheat Sheet Series, or the official Playwright documentation, use the `context7 mcp`. This ensures you are referencing up-to-date and authoritative sources for security and testing best practices.
Workspace Documentation
â— All project documentation should be maintained in the graph knowledge memory. Use the `graph knowledge memory mcp` to write, update, or delete documentation. This keeps documentation in sync with the codebase.

âœ… Automated Testing Guidelines
Test Scope
â— Create positive and negative tests to ensure comprehensive coverage. Focus on critical user paths and common error states.
â— One positive test per feature is sufficient unless more comprehensive testing is specifically requested.
Test Execution
â— Do not display Playwright reports in the UI.
â— Run tests in the background using terminal commands.
â— Analyze test results programmatically and provide a summary of outcomes.
â— Only show test failures or issues that need attention.
Test Structure
â— Use clear, descriptive test names that indicate the feature being tested.
â— Include proper test setup (`beforeEach`) and cleanup (`afterEach`) to ensure test isolation.
â— Use reliable selectors, prioritizing `data-testid` attributes to decouple tests from DOM structure changes.
â— Keep tests focused and atomic â€“ one test per specific functionality.
â— Extract shared test logic (e.g., login flows, form filling) into reusable helpers.
â— Use Page Object Models (POM) or test abstraction layers for interacting with the UI.
â— Maintain a consistent naming convention for `data-testid` attributes to simplify selection.
Test Names & Organization
â— Name test files after the feature they are testing (e.g., `user-login.spec.ts`) and not generic names like â€œbug-fixâ€ or â€œnew-featureâ€.
â— Group tests by feature or domain in clearly named folders.
â— Avoid duplicating similar test steps across different files.
â— Ensure shared flows are placed in a common location (e.g., `utils/testHelpers.ts`).

waiting Strategy
â— Avoid fixed waits (`page.waitForTimeout()`) as they lead to flaky and slow tests.
â— Use Playwright's auto-waiting mechanisms and web-first assertions (`expect(locator).toBeVisible()`) which are reliable and efficient.
â— For complex scenarios, wait for specific network responses (`page.waitForResponse()`) or load states (`page.waitForLoadState('networkidle')`) instead of arbitrary delays.

Test Data Management
â— Avoid hardcoding test data directly in test files.
â— Use test data factories to generate dynamic and unique data for each test run, ensuring test independence.
â— Implement cleanup strategies using `afterEach` or `afterAll` hooks to remove test data after execution, preventing state leakage.

ğŸ” Code Review Guidelines
Change Validation
â— Always review all changes holistically across the entire codebase.
â— Ensure changes are consistent across all related files.
â— Verify that modifications donâ€™t break existing functionality.
â— Check for orphaned references when files are deleted or renamed.
â— Identify and eliminate duplicated logic across test and utility files.
â— Review for repeated selectors or assertions that can be abstracted.
â— Ensure that no hardcoded URLs, credentials, or other sensitive data are present in the code.
â— Verify that robust waiting strategies are used in place of fixed timeouts.
Documentation Consistency
â— If files are deleted, edit the `README.md` or relevant files in the `/docs` directory to remove any references to them.
â— If new files are created, update the `README.md` or other relevant documentation to include information about the new files and their purpose.
â— Update project structure diagrams if significant changes are made (e.g., in `docs/architecture.md`).
â— Keep `package.json` scripts aligned with actual test files.
Integration Checks
â— Verify imports/exports are updated when files are moved or renamed.
â— Check that routing still works after page component changes.
â— Ensure CSS classes and styling remain consistent.
â— Validate that `data-testid` attributes are properly added for new interactive elements.

ğŸ§± Implementation Standards
Component Development
â— Add appropriate `data-testid` attributes for all testable elements to ensure stable selectors.
â— Maintain consistent styling and UX patterns.
â— Follow existing code patterns and conventions.
â— Prefer stronger TypeScript typing.
â— Reuse existing UI components instead of creating new ones for the same behavior.
â— Create and document shared logic in common utility files.
Feature Implementation
â— Implement features incrementally and test at each stage.
â— Maintain backwards compatibility unless breaking changes are intentional.
â— Evaluate the performance impact of new features.
â— Follow accessibility best practices. Consult relevant guidelines using the `context7 mcp` if needed.
â— Ensure all implementations adhere to security best practices. Use the `context7 mcp` to consult the OWASP ASVS or relevant cheat sheets.
â— All features should prioritize performance and be as fast as possible.

â™»ï¸ Code & Test Reusability
â— Extract shared test flows and utilities into central helper modules.
â— Use Page Object Model (POM) patterns to encapsulate UI behavior. Introduce a `BasePage` for shared logic and structure POMs in a layered architecture (e.g., Pages â†’ Components â†’ Utilities).
â— Ensure consistent naming and structure across components and tests.
â— Avoid writing the same assertions or flows in multiple placesâ€”refactor them.

ğŸŒ Environment & Configuration
â— Centralize all environment-specific configurations (e.g., base URLs, API endpoints) in a dedicated configuration file that reads from environment variables.
â— Do not hardcode URLs or credentials in test files.
â— Use `.env` files to manage environment variables for local development.

ğŸš€ Performance & CI/CD
â— Configure Playwright to run tests in parallel to reduce execution time in CI/CD pipelines.
â— Use test sharding to distribute large test suites across multiple machines for faster feedback.
â— Optimize CI configurations to only run necessary steps. For example, install only the required browsers.
â— Configure test retries in the `playwright.config.ts` to handle flaky tests in CI.
â— Generate and store test artifacts like traces, videos, and screenshots only on failure to save resources.

Tools allowed to use
â— Use Playwrightâ€™s built-in utilities and methods.
â— Use the provided MCP playwright server to access the application.
â— Use the `playwright mcp` for manual browser interaction and inspection.
â— Use the `sequentially mcp` to structure and plan task execution.
â— Use the `context7 mcp` to retrieve technical documentation (e.g., OWASP, Playwright docs).
â— Use the `graph knowledge memory mcp` to manage documentation.